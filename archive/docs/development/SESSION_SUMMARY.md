# PhonoLex Session Summary

## What We Built

A **transformer-based phonological embedding system** (PhonoLex-BERT) that learns contextual phoneme representations and word-level embeddings.

## Architecture Evolution

### 1. Initial Attempts (Failed)
- âŒ **Word-pair similarity**: Random pairs with heuristic similarity â†’ terrible results
- âŒ **Skip-gram**: Bag-of-phonemes â†’ cat/act similarity 0.99 (order ignored!)

### 2. Sequential LSTM (Good)
- âœ… Word embedding predicts phoneme sequence
- âœ… cat/act: 0.21 (order matters!)
- âš ï¸ But: No phoneme-level context

### 3. Contextual Transformer (Better)
- âœ… Each phoneme gets contextual embedding
- âœ… 92.8% masked phoneme prediction
- âš ï¸ But: Mean pooling loses position info

### 4. Position-Aware Aggregation (Best Baseline)
- âœ… Concat first-middle-last: cat/act = 0.31
- âœ… Rhymes work: cat/bat = 0.75
- âœ… Order preserved!

### 5. PhonoLex-BERT v1 (With Improvements)
- âœ… Contrastive learning (22K positive, 37K negative pairs)
- âœ… Attention pooling (learned aggregation)
- âš ï¸ But: All similarities too high (cat/phone = 0.87)

### 6. PhonoLex-BERT v2 (Current - Grid Search)
- âœ… More hard negatives (anagrams as negatives!)
- âœ… Graded similarity targets (not just 0/1)
- âœ… Grid search over: contra_weight Ã— temperature
- â³ Currently training...

## Key Insights

### 1. Phonology = Sentence, Phoneme = Word
```
NLP                         Phonology
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Sentence                 â†’  Word
Word                     â†’  Phoneme
Contextual word emb      â†’  Contextual phoneme emb
Sentence embedding       â†’  Word embedding (pooled)
```

### 2. Position Matters!
- Skip-gram treats phonemes as bag â†’ cat/act indistinguishable
- Transformers with positional encoding â†’ context-aware
- But mean/max pooling loses position!
- Solution: Concat positional segments OR attention pooling

### 3. Contrastive Learning is Critical
- MLM alone doesn't teach word-level similarity
- Need explicit positive/negative pairs:
  - **Positive**: rhymes, morphology, minimal pairs
  - **Negative**: unrelated, **anagrams** (critical!)

### 4. Anagram Test is Key Metric
cat /kÃ¦t/ vs act /Ã¦kt/ must have **LOW** similarity despite same phonemes!

## Data Sources

- **CMU Dict**: 125,764 words with ARPAbet â†’ IPA
- **SIGMORPHON**: 80,865 morphological pairs (19K with pronunciations)
- **Phoible**: English phoneme inventory (39 phonemes)
- **IPA-dict**: Additional pronunciations
- **UniMorph**: Derivational morphology

## Training Data Generated

### Positive Pairs (~22K)
- **Rhymes**: Same last 2+ phonemes (cat/bat)
- **Morphology**: Lemma + inflection (run/running)
- **Minimal pairs**: Edit distance = 1 (cat/bat)

### Negative Pairs (~37K)
- **Unrelated**: No phoneme overlap, different rhyme
- **Anagrams**: Same phonemes, different order (cat/act) â† critical!
- **Non-rhyming**: Some overlap but different endings

## Models Trained

```
models/
â”œâ”€â”€ word_embeddings/              # First attempt (failed)
â”œâ”€â”€ word_embeddings_skipgram/     # Skip-gram (order-invariant)
â”œâ”€â”€ word_embeddings_sequential/   # LSTM-based (good)
â”œâ”€â”€ contextual_phoneme_embeddings/ # Transformer baseline
â”œâ”€â”€ phonolex_bert/                # v1: Contrastive + Attention
â””â”€â”€ phonolex_bert_v2/             # v2: Improved (grid searching...)
```

## Results Progression

| Model | cat-bat | cat-act | cat-dog | run-running |
|-------|---------|---------|---------|-------------|
| Skip-gram | 0.67 | 0.99 âŒ | -0.33 | 0.76 |
| Sequential | N/A | 0.21 âœ… | N/A | N/A |
| Contextual (mean) | 0.77 | 0.94 âŒ | 0.17 | 0.82 |
| Concat (first-mid-last) | 0.75 | 0.31 âœ… | 0.23 | 0.57 |
| PhonoLex-BERT v1 | 0.98 | 0.86 âš ï¸ | 0.70 âš ï¸ | 0.97 |
| PhonoLex-BERT v2 | â³ Grid searching... |

## Grid Search Parameters

Testing 16 combinations:
- `contra_weight`: [0.5, 1.0, 2.0, 3.0]
- `temperature`: [0.03, 0.05, 0.07, 0.1]
- `epochs`: 8 (fast iteration)

**Goal**: Find config that:
- âœ… High similarity for rhymes/morphology
- âœ… Low similarity for unrelated/anagrams

**Score function**:
```
score = rhyme_sim + morph_sim - unrelated_sim - anagram_sim
```

## Code Files

### Training Scripts
- `train_word_embeddings_mps.py` - Initial attempt
- `train_word_embeddings_skipgram.py` - Skip-gram
- `train_word_embeddings_sequential.py` - LSTM sequence model
- `train_contextual_phoneme_embeddings.py` - Transformer MLM
- `train_phonology_bert.py` - v1 with contrastive learning
- `train_phonology_bert_v2.py` - v2 with improvements
- **`gridsearch_phonology_bert.py`** - Hyperparameter search â­

### Evaluation Scripts
- `evaluate_contextual_embeddings.py` - Test pooling methods
- `position_aware_word_embeddings.py` - Position-aware aggregation

### Data Loaders
- `src/phonolex/embeddings/english_data_loader.py` - English phonology data

### Documentation
- `IMPROVEMENTS.md` - Future enhancements
- `SESSION_SUMMARY.md` - This file

## Next Steps (After Grid Search)

### High Priority
1. âœ… Contrastive learning - **DONE**
2. âœ… Attention pooling - **DONE**
3. â³ **Tune hyperparameters** - IN PROGRESS
4. ğŸ”² Syllable structure awareness
   - Add onset-nucleus-coda decomposition
   - Separate embeddings for syllable positions

### Medium Priority
5. Multi-task learning
   - Stress prediction
   - Phonotactic validity
   - Rhyme classification
6. Phonological feature integration
   - Initialize with Phoible features
   - [voiceless, dorsal, stop] etc.

### Research-Level
7. Cross-lingual pre-training (2,716 languages!)
8. Sub-phoneme feature attention
9. Variational embeddings

## Applications

Current model ready for:
- Rhyme detection
- Phonological similarity search
- Morphological analysis
- Allomorph prediction
- Phoneme-aware spell correction
- Poetry generation (rhyme schemes)
- Linguistic analysis

## Performance

- **Training speed**: ~60s per epoch (125K words, MPS GPU)
- **MLM accuracy**: 90.5%
- **Model size**: 128-dim embeddings, 3-layer transformer
- **Parameters**: ~2M trainable parameters

## Key Takeaways

1. **Position matters in phonology** - can't just bag-of-phonemes
2. **Contrastive learning essential** - MLM alone insufficient
3. **Anagrams are hard negatives** - explicitly push them apart
4. **Fast iteration wins** - grid search finds best config quickly
5. **Transformer architecture** maps perfectly to phonology
